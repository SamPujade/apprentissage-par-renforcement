{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Notebook - Apprentissage par renforcement.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["hhFb-GqJrPv4","aZftsIswzFA3"],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hnPZSdjbrPv2","colab_type":"text"},"source":["# Apprentissage par renforcement"]},{"cell_type":"markdown","metadata":{"id":"hhFb-GqJrPv4","colab_type":"text"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"AmeKfT7arPv6","colab_type":"text"},"source":["L’apprentissage par renforcement est une discipline de l’intelligence artificielle, visant à faire évoluer un agent dans son environnement afin qu’il apprenne par lui-même à le maîtriser. Cette discipline est inspirée des neurosciences et de la psychologie expérimentale que l’on a adaptée à l’informatique, au début des années 1980. En effet les neurosciences ont mis à jour le renforcement des poids synaptiques des transmissions neuronales lorsque son activité devenait plus importante. D’un autre côté, l’expérience anthropologique a permis de découvrir des modèles de conditionnement chez les animaux, reposant sur la satisfaction. C’est la base du dressage : on stimule une douleur en cas d’échec et un plaisir en cas de succès. \n","\n","![Schéma_renforcement](https://image.noelshack.com/fichiers/2019/23/2/1559676471-whatsapp-image-2019-04-17-at-10-21-35.jpeg)\n","\n","C’est sur ces principes que se base en partie l’apprentissage par renforcement. Au début, l’algorithme est naïf et sans expérience. Il va évoluer de manière aléatoire dans son environnement. Ses évolutions sont évaluées par une fonction. L’algorithme est alors puni ou récompensé selon son action, et apprend. A l’image de l’expérience d’un rat dans une cage qui se voit assigné une croquette sécrétant de la dopamine ou un choc électrique stimulant la douleur, selon le levier qu’il presse.\n","\n","C’est ainsi que les suites d’actions menant à la somme des récompenses la plus élevée sont retenues et exploitées, tout comme les poids synaptiques s’accentuent lors d’une activité importante entre deux neurones. Les chaines d’actions menant aux meilleurs résultats sont priorisées par la suite.\n","\n","Tout au long de ce cours, nous utiliserons un exemple pour illustrer les différents concepts abordés: **le jeu du labyrinthe**.\n","\n","![Matrice Jeu ](https://image.noelshack.com/fichiers/2019/23/2/1559676166-grille-labyrinthe.png)\n","\n","Le principe est simple:\n","* la case bleue représente la case départ\n","* le joueur évolue à l'aveugle dans la grille\n","* le joueur perd s'il arrive dans la case rouge\n","* le joueur gagne s'il arrive dans la case jaune\n","\n","Le but est d'arriver à entraîner une IA pour trouver le chemin idéal dans le labyrinthe.\n"]},{"cell_type":"markdown","metadata":{"id":"MQosGJrQrPv6","colab_type":"text"},"source":["## 1 - Processus Markoviens"]},{"cell_type":"markdown","metadata":{"id":"jYscs9ZhrPv7","colab_type":"text"},"source":["Le formalisme des processus markoviens fournit un cadre précis et adapté à l'abstraction des techniques d'apprentissage par renforcement. N'hésitez pas à revenir à ce paragraphe régulièrement si vous n'êtes pas sûr de la définition d'un terme par exemple.\n","\n","###Formalisme\n","**Niveau 1 :**\n","\n","Dans l’apprentissage par renforcement, l’environnement est modélisé par un ensemble d’états. L’agent peut passer d’un état à un autre en effectuant une action. L’environnement renforce l’agent après chaque action en lui renvoyant une récompense positive ou négative. C’est ce qui permet « d’éduquer » l’agent. Le but de l’agent est de maximiser cette récompense.\n","\n","\n","* **AGENT** : c’est l’agent qui fait des actions et qui explore son environnement ; par exemple un robot qui se déplace dans notre labyrinthe, un personnage de jeu vidéo …\n","\n","![agent](https://image.noelshack.com/fichiers/2019/23/2/1559676615-agent.png)\n","* **ETAT $s \\in S$** : $S$ est l’ensemble des états du système. Un état $s$ correspond à la position où se trouve l’agent, ainsi qu'éventuellement d'autres caractéristiques pertinentes (barre de vie, munitions pour un jeu vidéo par exemple)\n","\n","![etat](https://image.noelshack.com/fichiers/2019/23/2/1559677041-etat.png)\n","\n","* **ACTION $a \\in A$** : $A$ est l’ensemble de toutes les actions que l’agent peut faire. $a$ est une action de l’ensemble $A$, elle peut être par exemple pour le robot: se déplacer à droite, se déplacer à gauche …\n","\n","![action](https://image.noelshack.com/fichiers/2019/23/2/1559676990-action.png)\n","\n","* **ENVIRONNEMENT** : c’est le cadre d’action de l’agent, le labyrinthe dans notre exemple. L’environnement prend l’état de l’agent et une action pour retourner une récompense et un nouveau état.\n","\n","![environnement](https://image.noelshack.com/fichiers/2019/23/2/1559677235-environnement.png)\n","* **RECOMPENSE $r \\in R$** : $R$ est l'ensemble des récompenses possibles. Une récompense $r$ correspond à l’évaluation des actions de l’agent. Dans notre exemple, le robot est récompensé (+100) s'il arrive dans la case jaune, et est pénalisé s'il arrive dans la case rouge (-100).\n","\n","![recompense](https://image.noelshack.com/fichiers/2019/23/2/1559677462-recompense.png)\n","* **POLITIQUE $\\pi$** : (avant tout un francissisme de \"policy\") c’est la stratégie que l’agent utilise pour déterminer ses actions futures selon son expérience passée et son état présent et qui garantie la meilleure récompense. $\\pi (s,a)$ est la probabilité que l'agent dans l'état s effectue l'action a.\n","\n","\n","Nous utilisons pour cela un outil de représentation qui s’appelle processus de Markov à temps discret ou bien chaine de Markov. Il permet de modéliser les différents états d’un système, accompagné des transitions possibles entre ces états.\n","\n","Exemple :\n","\n","![MDP](https://image.noelshack.com/fichiers/2019/23/2/1559677901-mdp.png)\n","\n","Dans une chaine de Markov chaque sommet représente un état et chaque arrête une transition possible entre les deux états. Le poids de l’arrête représente la probabilité que cette transition ait lieu. Dans le cadre de l’apprentissage par renforcement on associe également une récompense à chaque action effectuée dans un certain état. C’est ce qui permettra « d’éduquer » l’algorithme.\n","\n","**Niveau 2 :**\n","\n","Définissons ce processus de Markov de manière plus formelle :\n","* $S$ est un ensemble fini d’états discrets notés $s$ (ce sont les sommets du graphe).\n","* $A$ est un ensemble fini d’actions $a$ (c’est l’ensemble des arcs orientés).\n","* $T : S × A × S → [0 ; 1]$ est une fonction de transition associant la probabilité de transition de l’état s vers un autre état par une action de A.\n","* $R : S × A → R$,  représente la récompense d’effectuer l’action a dans l’état s, c’est une valeur réelle symbolisant une récompense ou une punition. (Ex : valeur positive ou négative)\n","\n","Dans le cadre de l’apprentissage par renforcement, les probabilités de transition ne sont pas connues de l'agent qui apprend.\n","\n","Si l’on revient a notre exemple précédent, nous aurons :\n","\n"," * $S=${ 1, 2, 3 ,4 ,5, Etat Terminal }.\n"," * $A=$ {Haut, Droite, Bas, Gauche}\n","\n","Dans l’état 1, les seules actions possibles sont {Haut , Droite} et on pourrait avoir :\n"," * $T(1, \\text{Haut})=0.5$ \n"," * $T(1, \\text{Droite})=0.5$ \n","\n","Nécessairement $T(1, \\text{Gauche})=0$ et $T(1, \\text{Bas})=0$. Les deux actions Haut et Droite sont équiprobables.\n","   \n","Exemple d’une fonction de récompense :  \n","$R(1, \\text{Haut})= -10$ \t(c’est le puits) \\\n","$R(3,\\text{Droite})= +10 $ (c’est la sortie) \\\n","$R(1,\\text{Droite})= -1 $  (c’est une case sans effet, on lui associe neanmoins une récompense négative afin de minimiser le nombre de coups)\n","\n","![MDP2](https://image.noelshack.com/fichiers/2019/23/3/1559760644-mdp2.png)\n","\n","Les fonctions de transition $T$ et de récompense $R$ permettent de définir complétement le problème et font partie de la donnée de l'environnement. Toutefois cette représentation ne permet pas de choisir l’action $a$ qui maximisera la récompense de l’agent lorsqu’il est dans l’état $s$. \n","On utilise pour ça une politique $\\pi$ et une fonction de valeur $V$.\n","\n","\n","\n","###Principe formel de l'apprentissage par renforcement\n","**Niveau 1**\n","\n","L’apprentissage par renforcement permet d’évaluer une politique $\\pi$, c’est-à-dire une stratégie de prise de décision. Pour cela, on utilise une fonction de valeur $V$. Cette fonction permet de \"juger\" chaque état, en leur associant une valeur. Plus cette valeur est grande, plus l'agent a intérêt à aller dans cet état. À l'inverse, plus cette valeur est faible, plus il a intérêt à l'éviter. Pendant l'apprentissage, l'agent va modifier la fonction de valeur pour la rendre optimale.\n","\n","Les étapes sont les suivantes :\n","\n","* On fixe la politique $\\pi$ à évaluer\n","* On initialise la fonction de valeur $V^{\\pi}$\n","* L’agent évolue dans son environnement. A chaque étape :\n","\t- l’agent est dans un état k\n","\t- une action est déterminée par la politique choisie\n","\t- l’agent arrive dans l’état k+1 et reçoit la récompense relative à cet état\n","\t- on actualise la fonction de valeur $V^{\\pi}$\n","* On répète l’étape précédente jusqu’à obtenir une fonction de valeur $V^{\\pi}$ satisfaisante (ex : qui ne change plus trop malgré de nouvelles parties, c'est-à-dire qu'elle a convergé, même si elle est mauvaise, auquel cas on réfléchit comment mieux faire apprendre)\n","\n","Le concept de fonction de valeur est traité plus en détail dans la partie 2.\n","\n","**Niveau 2**\n","\n","Dans le cas d'un problème décisionnel dit à horizon temporel fini n, c'est-à-dire quand l'interaction entre l'agent et l'environnement se termine au bout de $n<\\infty$ itérations, la fonction de valeur $V$ évaluant la politique $\\pi$ est définie ainsi:\n","\n","$$ V^{\\pi}(s) = \\mathbb{E}^{\\pi} \\begin{bmatrix} \\sum_{t=0}^{n-1} r(s_{t},a_{t})|s_{0}=s\\end{bmatrix}  $$\n","\n","avec $ \\mathbb{E}^{\\pi}$ l'espérance lorsque l'agent suit la politique $\\pi$.\n","\n","Pendant l'apprentissage, l'objectif de l'agent est de déterminer une politique qui lui permette de gagner la plus grande récompense possible en maximisant la fonction de valeur. \n","\n","On dit qu’une politique $\\pi^{*} $ est *optimale* si elle vérifie:\n","$$ \\forall \\pi, \\forall x \\in X, V^{\\pi}(x) \\leq V^{\\pi^{*}}(x)$$\n","\n","On peut montrer que comme les états et les actions sont en nombre fini, il existe bien  une telle politique optimale déterministe. L'objectif est donc de trouver pendant l'apprentissage une politique $\\pi$ se rapprochant le plus possible de cette politique optimale $\\pi^{*}$.\n","\n","Cependant, la fonction de valeur $V$ n'est pas forcément bien définie de la manière précédente dans le cas d'un problème décissionnel à horizon temporel infini, comme c'est le cas pour le jeu du labyrinthe (l'agent peut très bien se déplacer sur la grille indéfiniment sans rencontrer un état terminal, la cause jaune ou la case rouge). En effet, la somme $\\sum_{t=0}^{\\infty} r(s_{t},a_{t})$ peut très bien diverger. Pour cela, on introduit un *facteur de pondération $\\gamma \\in [0;1[$*. Comme la fonction de récompense $r$ est bornée, on a bien cette fois ci la convergence de la somme $\\sum_{t=0}^{\\infty} \\gamma^{t}r(s_{t},a_{t})$. On définie alors la fonction de valeur ainsi:\n","\n","$$ V_{\\gamma}^{\\pi}(s) = \\mathbb{E}^{\\pi} \\begin{bmatrix} \\sum_{t=0}^{\\infty}\\gamma^{t} r(s_{t},a_{t})|s_{0}=s\\end{bmatrix}  $$\n"]},{"cell_type":"markdown","metadata":{"id":"Nmv9c_L5rPv8","colab_type":"text"},"source":["## 2- TD-Learning, Q Learning"]},{"cell_type":"markdown","metadata":{"id":"Dgbth2L3rPv8","colab_type":"text"},"source":["### Fonction de valeur\n","\n","**Niveau 1**\n","\n","Pour que notre agent (donc en pratique : notre algorithme) apprenne, il faut le doter d'une mémoire de manière à ce qu'il puisse se souvenir de toutes les récompenses ou punitions qu'il a reçu durant ses précédentes parties. C'est là qu'entre en jeu la fonction de valeur : elle permet à l'agent d'avoir accès rapidement à une information sur son environnement qui a été créée par son expérience seule. Comme il la recalcule fréquemment (en général et dans le cadre de ce notebook, après chaque action) elle représente assez fidèlement (pour qui sait l'interpréter) les connaissances que l'agent a appris de son environnement.\n","Mais qu'est-ce qu'une fonction de valeur ? \n","\n","En pratique, comme on le verra, c'est (dans le cas le plus simple) une fonction qui associe à chaque état une valeur qui représente l'intérêt de cet état pour l'agent. Ce n'est pas tant la récompense immédiate donnée par cet état qui est prise en compte, mais plutôt les récompenses futures que l'état pourra amener. Ainsi lorsque notre agent a bien appris après plusieurs entraînements, il se laisse guider par sa fonction de valeur qui le mène vers un chemin optimal pour lui (mais parfois, malgré un entraînement long, l'agent peut rater des choses intéressantes, surtout si l'environnement est grand).\n","\n","Encore plus en pratique, si notre jeu n'est pas trop gros (nombre d'états réduit, par exemple : ni un jeu vidéo ni une partie d'échec), la fonction de valeur est juste une double liste : une liste d'états, et la liste des valeurs de ces états.\n","\n","Exemple :\n","\n","|Etat #1\t| Etat #2\t| Etat #3\t| Etat #4 |\n","|-|-|-|-|\n","|V(1) | V(2)\t| V(3)\t| V(4) |\n","\n","\n","On peut appliquer cette méthode à notre labyrinthe:\n","\n","![gif_V_function](https://media.giphy.com/media/Kx7LllAPfimFar0u8A/giphy.gif)\n","\n","Ici, les numéros dans chaque case indiquent la fonction de valeur appliquée à l'état correspondant à la case. Les flèches pointent la case adjacente avec la valeur la plus élevée.\n","\n","\n","**Niveau 2**\n","\n","Le TD-Learning (Temporal Difference Learning) et le Q-Learning sont deux algorithmes d'apprentissage par renforcement qui fonctionnent avec une fonction de valeur qui caractérise la récompense maximale que le joueur peut obtenir selon ses actions. \n","Plus précisément, d’un côté, le TD-learning fonctionne avec une fonction de valeur V qui attribue un nombre à chaque état. Plus ce nombre est grand, plus l'état est intéressant. Ainsi après avoir précisé sa propre fonction de valeur (qu'il initialise à 0 partout, ou aléatoirement) en jouant de multiples parties, l'agent tend vers une fonction de valeur unique. Lorsqu'il détermine qu'il a suffisamment bien précisé sa fonction de valeur (tellement bien que malgré les autres parties d'entraînement, elle ne change quasiment plus), il a fini l'entraînement et est capable de joueur optimalement (selon lui). Jouer optimalement consiste à partir de chaque état à aller vers l'état avec la plus grande valeur, dans l'espoir d'obtenir les plus grande récompenses (la fonction de valeur de chaque état ne correspond pas à la récompense de cet état, mais en donne une estimation et surtout donne une estimation des récompenses futures possibles après cet état)\n","De l'autre côté, le Q-learning utilise une fonction similaire Q, qui attribue un nombre à chaque couple $(\\text{etat}, \\text{action})$ : cela signifie que chaque action est notée individuellement depuis chaque état, selon le même principe que précédemment. Donc jouer optimalement avec une fonction Q bien déterminée après un entraînement consiste à : depuis chaque état, jouer l'action avec la plus grosse valeur (Q-learning) au lieu de : depuis chaque état, jouer l'action permettant d'aller dans l'état de plus grosse valeur. La différence est subtile, mais elle se traduit par des différences de comportement pouvant être embêtantes sur des jeux spécifiques.\n","\n","*Exemple :  Dans les jeux non-déterministes (jeu où une certaine action depuis un certain état ne donne pas toujours le même résultat), il est plus intéressant de savoir quel est l'intérêt de jouer une certaine action plutôt que de savoir vers quel état aller, sachant qu'on ne pourra pas y aller de manière certaine.* \n","\n","On voit donc que le fait de noter les couples $(\\text{etat}, \\text{action})$ plutôt que simplement les états est une méthode plus générale : pour ces raisons, nous allons nous concentrer à présent sur le Q-Learning dans la suite du cours (sauf pour le niveau 1 suivant car son cadre correspond à celui du TD(0), un certain type de TD-Learning (le plus simple) )\n","\n","Enfin, il existe un autre type d'algorithme (en fait, beaucoup d'autres) appelé SARSA et très proche du QLearning, qui différe légèrement dans sa manière d'actualiser la fonction de valeur et qui a l'avantage pratique de faire joueur de manière plus sûre quoique parfois moins optimale par rapport au QLearning classique (quelle que soit la stratégie associée, voir partie suivante).\n","Voir ce lien pour une comparaison intéressante : http://www-igm.univ-mlv.fr/~dr/XPOSE2014/Machin_Learning/A_Exemple.html\n","\n","### Comment entraîner cette fonction de valeur ?\n","\n","**Niveau 1**\n","\n","Reprenons le cas d'un jeu pas trop gros, et pour ceux qui ont lu le niveau 2 précédent, plaçons-nous dans le cadre de l'algorithme TD(0) : cela signifie que comme dans le niveau 1 précédent, notre fonction de valeur associe un nombre à chaque état, sous la forme d’un double liste, ou d’un tableau (comme vous préférez l’imaginer) puisque notre jeu n’est pas trop gros. Le principe étant que plus la valeur d’un état est grande, plus l’état est intéressant, découvrons comment actualiser la fonction de valeur : \n","Supposons que l’on part de l’état X pour aller dans l’état Y, en obtenant une récompense R. On veut actualiser la valeur associée à l’état X. Pour cela, la recette est la suivante : gardons un peu de l’ancienne valeur, ajoutons-y une dose de la récompense obtenue, et un petit peu de la valeur de Y pour avoir une idée de ce qui nous attend dans le futur. \n","Ecrivons un peu mieux cette recette avec la sauce mathématique : \n","(même si vous avez fait maths spé, l'interprétation verbale de la formule qui arrive vaut le coup)\n","-\tAppelons 0≤α≤1 le pourcentage d’importance accordé à tout ce qui est nouveau dans l’actualisation (pas l’ancienne valeur de l’état) : c’est notre bien-nommé taux d’apprentissage.\n","-\tAppelons 0≤γ≤1 le rapport dans lequel on prend en considération la valeur de Y pour actualiser la valeur de X dans notre exemple. C’est un paramètre anti-myopie : à 0, on actualise la valeur de X qu’avec la récompense que l’on a obtenue en allant sur Y, et on ne prend donc pas en compte le futur que promet Y. A 1, en revanche, on le prend beaucoup en compte, ainsi que par récurrence les valeurs des états suivants puisque Y les prend aussi en compte dans sa valeur … (tant pis si vous ne comprenez pas parfaitement ça en première lecture)\n","Cela nous donne donc :\n","\n","$Valeur(X) = (1- \\alpha) \\times Valeur(X) + \\alpha \\times(R + \\gamma \\times Valeur(Y))$\n","\n","\n","\n","**Niveau 2**\n","\n","Plaçons-nous ici aussi dans le cadre d’un jeu pas trop gros, donc notre fonction de valeur de l’algorithme de Q-learning est un tableau prenant en paramètres les états et actions, et sera appelée dans toute la suite et dans les programmes QTable. Bien sûr, toutes les actions ne sont pas toujours possibles depuis chaque état. Enfin, même si informatiquement notre QTable ne sera pas toujours un tableau mais parfois une matrice 3x3 (correspondant aux états) de dictionnaires (actions dans ces états et valeurs), on continuera de l’appeler QTable\n","\n","Exemple très simple : \n","\n","| |Etat #1|Etat #2\t|Etat #3|\n","|-|-|-|-|\n","|Action #1|\t-\t|Q(2, 1)|Q(3, 1)|\n","|Action #2|Q(1, 2)|\tQ(2, 2)|Q(3, 2)|\n","|Action #3|Q(1, 3)| - |Q(3 , 3)|\n","\n","\n","L'initialisation de la table de Q valeurs (QTable) peut se faire soit de manière aléatoire soit avec que des 0 (pour les actions possibles on n'initalise rien du tout pour les couples $(\\text{etat}, \\text{action})$ impossibles car la Q-valeur associée ne doit pas exister), par exemple. Sachez que cela n'a aucune importance si vos paramètres sont bien calibrés. Mais quel sont ces paramètres ? Il y en a deux, qui servent à gérer la manière dont on actualise la fonction de valeur. Ils doivent être fixés lorsqu'on entraîne un joueur de manière à converger vers une QTable unique. Le premier paramètre, couramment noté $\\alpha \\in \\left[0,1\\right]$ est le taux d'apprentissage. ll détermine à quel point les valeurs de la QTable vont être remplacées lorsqu'on les actualise. Le second, noté $\\gamma$, que l'on va appeler le taux de prévision (\"discount rate\" en anglais). En effet, il caractérise l'importance accordée aux actions futures dans le jeux. A 0, le joueur est myope, à 1, il accorde beaucoup d'importance au futur.\n","\n","Déterminons à présent que les paramètres sont définis la formule qui régit l'actualisation de la QTable. A chaque fois qu'un joueur effectue une action $a$ depuis un état $s$, on doit actualiser $Q(s, a)$ selon : son état de départ $s$, son état d'arrivée $s'$, et la récompense qu'il a obtenue en effectuant cette action $a$ (dans tous nos jeux, la récompense est associée à un état, donc ici c'est la récompense de l'état d'arrivée qui nous intéresse : $R(s')$ ). Nous avons dit dans la sous-partie précédente que la fonction de valeur (ou QTable) prenait non seulement en compte les récompenses imédiates mais aussi à quel point un état pouvait être intéressant dans le futur. Donc, la nouvelle valeur qu'on va appliquer à notre QTable doit prendre en compte d'un côté $R(s')$, et de l'autre, \"à quel point $s'$ est intéressant.\" Pour cela, on va utiliser la valeur $\\max_{a' \\in s'} Q(a', s')$ qui utilise notre fonction de valeur. Ce dernier terme va être modulé par $\\gamma$ donc, puisque c'est à cela qu'il sert, et ensuite, nous allons actualiser $Q(s, a)$ selon le principe suivant : $\\left(1-\\alpha\\right) \\times \\text{vieille valeur} + \\alpha \\times \\text{nouvelle valeur}$.\n","Donc si vous avez bien compris, la formule d'actualisation de la QTable est : \n","\n","$Q(s, a) = \\left(1-\\alpha\\right) \\times Q(s, a) + \\alpha \\times \\left(R(s') + \\gamma \\times \\max_{a' \\in s'} Q(a', s')\\right)$\n","\n","\n","### Synthèse et exemple\n","\n","Nous avons présenté dans cette partie comment un agent artificiel (un programme informatique quoi) détermine de manière évolutive l'intérêt de chaque action dans chaque état de l'environnement en utilisant une fonction de valeur, et en l’actualisant au fur et à mesure de son entraînement, de la même manière qu'un enfant fait : \n","\n","**Niveau 1 très rapide** : L’état « être devant la cheminée » est risqué : il peut mener à effectuer l’action « tendre la main » qui va apporter une récompense très négative, donc une fois que l’enfant aura expérimenté cela une fois (dans le cas ou le cerveau de l’enfant apprend selon la méthode TD(0)), il aura tendance à éviter l’état « être devant la cheminée » à l’avenir, même si cet état peut aussi apporter une récompense intéressante dans le cas de la suite d’action « pas bouger », « pas bouger », « pas bouger », liée au fait d’avoir chaud. Mais comme la sensation de chaleur procure moins de plaisir que le fait de se brûler provoque d’inconfort, globalement la valeur de l’état sera mauvaise … C’est bête, on doit pouvoir faire mieux comme algorithme ! Par exemple, en attribuant plutôt une valeur à un couple $(\\text{etat}, \\text{action})$, ce qui rendrait l’apprentissage plus précis non ? Tout à fait, c’est l’objet du QLearning et du niveau 2 de cette partie, menant à un tableau de valeurs pour les jeux pas trop gros ressemblant à ça : \n","\n","| |Etat #1|Etat #2\t|Etat #3|\n","|-|-|-|-|\n","|Action #1|\t-\t|Q(2, 1)|Q(3, 1)|\n","|Action #2|Q(1, 2)|\tQ(2, 2)|Q(3, 2)|\n","|Action #3|Q(1, 3)| - |Q(3 , 3)|\n","\n","Au lieu de ça :\n","\n","|Etat #1\t| Etat #2\t| Etat #3\t| Etat #4 |\n","|-|-|-|-|\n","|V(1) | V(2)\t| V(3)\t| V(4) |\n","\n","Et ça s’appelle une QTable. (Vous risquez d’en avoir besoin ensuite)\n","\n","**Niveau 2 très rapide** : Dans l'état \"être devant la cheminée\", l'action \"tendre la main\" apporte une récompense très négative, donc la valeur du couple associé diminue. Dans la partie suivante, nous découvrirons comment sont mises en oeuvre différentes stratégies pour que l’agent utilise optimalement sa QTable qu'il essaie de faire converger.\n"]},{"cell_type":"markdown","metadata":{"id":"1SScGY5OrPv9","colab_type":"text"},"source":["## 3- Dilemme Exploration/Exploitation"]},{"cell_type":"markdown","metadata":{"id":"pUkcgN19rPv-","colab_type":"text"},"source":["Alors que la partie 2 traitait de comment l'agent obtient des informations sur son environnement, nous allons ici voir comment l'agent se comporte dans son environnement grâce aux informations acquises.\n","\n","\n","###Phases comportementales et stratégies\n","**Niveau 1**\n","\n","Les méthodes TD-Learning et Q-Learning permettent donc d'évaluer les actions de l'agent dans son environnement grâce à une fonction de valeur pour pouvoir influencer son comportement futur. Il reste maintenant à définir comment l'agent tient compte de cela pour effectivement influencer son comportement.\n","\n","Pour cela, on distinguera deux phases différentes dans un stratégie comportementale donnée :\n","\n","* **L'exploration**: l'agent n'a aucune connaissance, il évolue de manière aléatoire dans son environnement.\n","* **L'exploitation**: l'agent utilise sa connaissance, il agit de manière optimale, selon la fonction de valeur qu'il cherchera à maximiser.\n","\n","Phase d'exploration:\n","\n","\n","![gif_exploration](https://media.giphy.com/media/iDHjpZws3TiLpmv9TM/giphy.gif)\n","\n","\n","\n","Phase d'exploitation:\n","\n","\n","![gif_exploitation](https://media.giphy.com/media/jRvfEiBdF7a5RwBmgD/giphy.gif)\n","\n","\n","Le dilemme est que l'on ne peut pas rester dans une seule de ces deux phases pour l'apprentissage par renforcement. En effet, l'exploitation agira certes de manière optimale mais ne permet pas d'apprendre, tandis que l'exploration de tous les parcours possibles serait trop longue. Il faut donc trouver un bon équilibre entre exploration et exploitation dans l'apprentissage afin que l'agent apprenne vite et bien.\n","\n","\n","\n","**Niveau 2**\n","\n","Formellement, une politique est une fonction, qui à un état $s$ et une action $a$ associe une probabilité, celle d'effectuer l'action $a$ dans l'état $s$. Définir la politique consiste donc à établir les probabilités pour chaque état d'effectuer telle ou telle action.\n"," \n","Voici deux méthodes pour établir une stratégie dans les algorithmes de TD-Learning et Q-Learning:\n","\n","\n","#### La méthode $\\epsilon$-gloutonne\n","\n","La méthode $\\epsilon$-greedy, ou $\\epsilon$-gloutonne, consiste à définir une probabilité $\\epsilon$ pour l'agent d'effectuer une action d'exploration, et donc une probabilité $1-\\epsilon$ d'effectuer une action d'exploitation (selon la fonction de valeur actuelle). Dans le cas de l'exploration, l'action est choisie de manière aléatoire, indépendamment de la fonction de valeur. Plus $\\epsilon$ est proche de 1, plus l'algorithme d'apprentissage est lent, mais la solution sera plus optimale. Avec $\\epsilon=1$, la méthode est dite gloutonne.\n","\n","\n","#### La méthode softmax\n","\n","Dans certains cas, il est préférable d'éviter les récompenses trop négatives, qui peuvent fausser l'estimation des états. Une solution est d'avoir une probabilité d'effectuer une action a dans l'état s proportionnelle à sa fonction de valeur $Q(s,a)$. Cela permet d'avoir le plus souvent des actions proches de la meilleure:\n","\n","$$\\pi(s,a)=Q(s,a)/∑_{b\\in A}Q(s,b)$$\n","\n","La méthode « softmax » est une amélioration de cette méthode. Elle permet notamment de gérer les cas où $Q$ est à valeur négative.  Elle choisit l’action $a$ avec une probabilité :\n","\n","$$\\pi(s,a)=exp(Q(s,a)/\\tau)/∑_{b\\in A}exp(Q(s,b)/\\tau))$$\n","\n","Le facteur $\\tau$ s'appelle la température, et permet de modifier la balance exploration/exploitation. En effet, si $\\tau$ est très grand, les exponnentielles valent toutes 1, et donc la politique suivra une distribution uniforme, donc un comportement d'exploration. A l'inverse, quand $\\tau$ tend vers 0, la méthode devient déterministe et on aura un comportement d'exploitation.\n","\n","\n","\n","### Comment gérer ces deux phases comportementales pour en tirer le meilleur ?\n","**Niveau 1**\n","\n","Dans la nature, un animal commence pendant son enfance par un comportement d'exploration. A sa naissance, il ne sait rien et agit de manière aveugle, il apprend petit à petit par essais/erreur. A l'âge adulte, l'animal sera moins téméraire, et agira de manière optimale en se basant sur son expérience. Une méthode d'application est de s'inspirer de la biologie et du comportement animal : cela consiste à régler les facteurs de manière à avoir plutôt beaucoup d'exploration en début d'apprentissage, suivi de plus en plus d'exploitation en fin d'apprentissage. Cela rend les algorithmes beaucoup plus efficaces. \n","\n","**Niveau 2**\n","\n","Le choix de la politique dépend beaucoup du cas d'application. Un des intérêts de ces méthodes est de pouvoir moduler la balance exploration/exploitation au cours de l'apprentissage, par le biais des facteurs $\\epsilon$ et $\\tau$.\n","Nous vous encourageons vivement à les tester vous-mêmes, selon votre problème l'une pourra se révéler bien meilleure ... La plus classique et triviale reste la politique $\\epsilon$-greedy mais les deux sont rapides à implémenter une fois que le reste est fait.\n"]},{"cell_type":"code","metadata":{"id":"QV-Nto5orPv-","colab_type":"code","outputId":"36419b6c-7d3c-46ad-b888-5b571a4c8be2","executionInfo":{"status":"error","timestamp":1559761716351,"user_tz":-120,"elapsed":2419,"user":{"displayName":"Samuel Pujade-Renaud","photoUrl":"","userId":"16476981216653334163"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["###CECI EST UN CODE PERMETTANT DE VOIR LA DIFFERENCE ENTRE UN COMPORTEMENT D'EXPLORATION ET UN COMPORTEMENT D'EXPLOITATION \n","###SUR UN JOuEUR DEJA BIEN ENTRAINE\n","\n","###S'IL NE FONCTIONNE PAS DANS LE NOTEBOOK COPIEZ-COLLEZ LE DANS UN FICHIER PYTHON 3.7, C'EST A CAUSE DE TKINTER\n","\n","###IL EST DERIVE D'UN CODE PLUS COMPLET PERMETTANT D'ENTRAINER ET DE FAIRE JOUER UN JOUEUR SUR N'IMPORTE QUEL JEU 2D\n","###ET PEUT ETRE ADAPTE A UN PROBLEME PLUS LARGE\n","\n","\n","#Un joueur évolue sur une grille 2D et doit apprendre à maximiser sa récompense totale en un nombre de coups limités.\n","#Certaines cases sont recompensées positivement, d'autres négativement et arrêtent la partie.\n","\n","#Le code complet est long mais très commenté est nous vous encourageons à le lire attentivement après la fin du cours \n","#et à le comprendre pour voir la mise en oeuvre pratique de ce qui a été vu. Il est fourni en annexe.\n","\n","\n","\n","from numpy import math #on importe le module math pour l'infini et sqrt\n","from random import choice, random\n","from tkinter import *\n","import time\n","\n","\n","\n","class environment():\n","    \"\"\"Une instance de cette classe est un environnement, un terrain de jeu, tout ce qui enveloppe le reste.\"\"\"\n","\n","    def __init__(self, n, rewarded_states=[], rewards=[], terminal_states=[], forbidden_states=[], start_point=[0, 0], max_plays = 30, move_penalty = -1):\n","\n","        self.states=[[] for k in range(n)] #la matrice des états de l'environnement = cases.\n","        #ATTENTION: pour suivre les coordonées Tkinter, c'est une liste de colonnes descendantes. x:gauche->droite ; y:haut->bas\n","\n","        self.max_plays = max_plays #le nombre de coups (actions) maximum autorisés par partie\n","        self.move_penalty = move_penalty #\n","        self.len=n #pratique pour aprèsla récompense fixe attribuée pour chaque action\n","\n","        #le choix de créer une liste d'états récompensés et une liste des récompenses\n","        #vient du fait qu'en général il y a peu d'état récompensés et donc\n","        #il serait lourd d'avoir juste une grosse matrice de récompense quasi-nulle\n","\n","\n","        #création des états (objets)\n","        for x in range(n):\n","            for y in range(n):\n","                reward=0\n","                is_terminal=False\n","                if [x, y] in rewarded_states:\n","                    indice=rewarded_states.index([x, y])\n","                    reward=rewards[indice]\n","                if [x, y] in terminal_states:\n","                    is_terminal=True\n","                self.states[x]+=[state(x, y, reward, is_terminal)]\n","\n","        #rensignement de l'état de départ de l'environnement\n","        self.start = self.states[start_point[0]][start_point[1]] #état de départ\n","\n","        #éliminations des actions possibles depuis chaque état selon position (bord) ou états interdits adjacents\n","        for x in range(n):\n","            for y in range(n):\n","                if x==0 or [x-1, y] in forbidden_states:\n","                    self.states[x][y].possible_actions.remove('Left')\n","                elif x==n-1 or [x+1, y] in forbidden_states:\n","                    self.states[x][y].possible_actions.remove('Right')\n","                if y==0 or [x, y-1] in forbidden_states:\n","                    self.states[x][y].possible_actions.remove('Up')\n","                elif y==n-1 or [x, y+1] in forbidden_states:\n","                    self.states[x][y].possible_actions.remove('Down')\n","\n","        #initialisation de l'interface graphique\n","        def color(self, s): #s c'est un state\n","            return 'red' if s.reward<0 else 'yellow' if s.reward>0 else 'black' if [s.x, s.y] in forbidden_states else 'blue' if s == self.start else None\n","\n","        #on a a besoin pour après, pas seulement en local\n","        self.size = int(600/n)*n\n","        self.step = self.size / n\n","\n","        self.root = Tk()\n","        self.root.title('Jeu 2D interactif')\n","\n","        self.canvas = Canvas(self.root, height=self.size+1+13, width=self.size+1) #le +1 c'est pour les lignes droite et bas\n","        #self.canvas.create_text(1, self.size+1,anchor='nw', text='Nombre maximal de coups : {} ; Penalité de mouvement : {}'.format(self.max_plays, self.move_penalty))\n","\n","        #on représente les états par des carrés adéquats sur une grille\n","        for i in range(n):\n","            for j in range(n):\n","                s=self.states[i][j]\n","                self.canvas.create_rectangle(2+i*self.step, 2+j*self.step, 2+(i+1)*self.step, 2+(j+1)*self.step, fill = color(self, s), width=1, tag = '%d_%d'%(i,j))\n","                if s.reward!=0:\n","                    self.canvas.create_text(2+(i+0.5)*self.step, 2+(j+0.5)*self.step, text='{}'.format(s.reward), anchor='center')\n","\n","        self.canvas.pack(side='left')\n","\n","        #dans un canvas tkinter, les coordonnées dessinables vont de 2 à n+1 où n est une dimension\n","\n","#cette classe est aussi utile qu'un dictionnaire qui aurait les mêmes attributs, mais c'est sympa comme ça\n","class state():\n","    \"\"\"Une instance de cette classe est un état de l'environnement, elle est donc générée automatiquement par un environnement\n","     Un objet état contient toutes les informations nécessaire relatives à lui-même : position sur la grille,\n","    actions possibles, récompense, terminalité.\"\"\"\n","\n","    def __init__(self, x, y, reward, is_terminal):\n","        self.x=x\n","        self.y=y\n","        self.reward=reward\n","        self.is_terminal=is_terminal\n","        self.possible_actions=['Left', 'Right', 'Up', 'Down']\n","\n","\n","#Un petit environnement sympathique\n","#E=environment(n=7, rewarded_states=[[0,0],[1,0],[6,0],[1,1],[5,1],[4,2],[4,6],[5,6]],\n","                #terminal_states=[[1,0],[6,0],[1,1],[4,2]], rewards=[100, -1000, -1000, -1000, 10, -1000, 2, 2], start_point=[1, 5])\n","\n","E=environment(n=4, rewarded_states=[[2, 0], [1, 1]], terminal_states=[[2, 0], [1, 1]], rewards=[100, -100], start_point=[0, 3])\n","\n","\n","\n","\n","class game():\n","    \"\"\"Une instance de cette classe simule une partie du jeu.\n","Dépend donc d'un objet environnement. Et d'un objet joueur capable de jouer beaucoup de parties.\n","Dispose de méthode permettant de faire avancer la partie en jouant selon diverses stratégies possibles.\"\"\"\n","\n","    def __init__(self, environnement, player, epsilon, tau):\n","        self.state = environnement.start #on commence la partie sur l'état de départ de l'environnement\n","        self.previous_state=None\n","        self.score = 0 #le score cumulatif qui va être réalisé sur la partie\n","        self.is_over = False #est-ce que la partie est finie\n","        self.deltamax = 0 #la valeur abs de la correction maximale effectuée sur la QTable du superobjet joueur pendant la partie\n","\n","        self.P = player\n","        self.e = epsilon\n","        self.tau = tau\n","        self.env = environnement\n","\n","        self.movements = 0 #le nombre de coups joués sur cette partie. Eh oui, l'environnement limite le nombre de coups jouables par partie.\n","\n","\n","    def play(self, learning=True, method='e_greedy'): #epsilon-greedy method or softmax or optimal (just play one of the best actions)\n","        \"\"\"Effectue une action en actualisant la QTable du joueur et le deltamax ou pas (selon la valeur de learning), et en actualisant le nombre de mouvements, le score et la terminalité.\"\"\"\n","\n","        if method=='e_greedy': #la méthode epsilon-greedy\n","            if random()<self.e:\n","                order = choice(self.state.possible_actions)\n","            else:\n","                order=choice(self.best_actions())\n","\n","        elif method=='softmax': #la méthode softmax\n","            QTotal = sum([exp(self.P.QTable[self.state.x][self.state.y][action]/self.tau) for action in self.state.possible_actions])\n","            if QTotal == 0:\n","                order = choice(self.state.possible_actions)\n","            else:\n","                Proba={} #dico de probas\n","                for action in self.state.possible_actions:\n","                    Proba[action]=exp( self.P.QTable[self.state.x][self.state.y][action] / self.tau ) / QTotal\n","                alea=random() #entre 0 et 1\n","                if alea<Proba['Left']:\n","                    order = 'Left'\n","                elif alea<Proba['Left']+Proba['Right']:\n","                    order = 'Right'\n","                elif alea<Proba['Left']+Proba['Right']+Proba['Up']:\n","                    order = 'Up'\n","                else:\n","                    order = 'Down'\n","                #pas de risque d'action impossible\n","\n","        elif method=='optimal': #action optimale\n","            order=choice(self.best_actions())\n","\n","\n","        #maintenant on gère ce qu'il y a à gérer pour exécuter l'ordre de jeu donné par la méthode choisie\n","        self.previous_state = self.state\n","        self.move(order)\n","        self.movements += 1\n","        self.score+=self.state.reward + self.env.move_penalty\n","\n","        if learning: #update de la QTable si learning==True\n","            old_value = self.P.QTable[self.previous_state.x][self.previous_state.y][order]\n","            self.P.update_QTable(self.previous_state, order, self.state, self.state.reward + self.env.move_penalty)\n","            correction = abs(old_value - self.P.QTable[self.previous_state.x][self.previous_state.y][order])\n","            if correction > self.deltamax :\n","                self.deltamax = correction\n","\n","        if self.state.is_terminal == True or self.movements==self.env.max_plays: #on vérifie qu'on a pas fini la partie\n","            self.is_over = True\n","\n","\n","    def best_actions(self):\n","        liste=[]\n","        Q=self.P.QTable[self.state.x][self.state.y]\n","        for action in self.state.possible_actions:\n","            if Q[action] == max(Q.values()): #tout fonctionne parce que les seules clés de Q sont des actions possibles depuis l'état\n","                liste+=[action]\n","        return liste\n","\n","\n","    def move(self, order):\n","        if order=='Left':\n","            self.state=self.env.states[self.state.x-1][self.state.y]\n","        elif order=='Right':\n","            self.state=self.env.states[self.state.x+1][self.state.y]\n","        elif order=='Up':\n","            self.state=self.env.states[self.state.x][self.state.y-1]\n","        elif order=='Down':\n","            self.state=self.env.states[self.state.x][self.state.y+1]\n","\n","\n","\n","class player():\n","    \"\"\"Un objet de cette classe va être un superobjet qui contiendra des informations\n","sur un entraînement réalisé avec une série de parties. Il représente donc un joueur virtuel qui apprend en jouant des parties successives.\n","La méthode train() va instancier\n","des objets game (= va créer des parties) et les faire jouer jusqu'à un\n","état terminal (=va jouer les parties jusqu'au bout) jusqu'à atteindre les critères de convergence.\n","De là, on pourra accéder à la QTable obtenue suite à cet entraînement, et faire\n","jouer des parties sans la modifier pour observer le comportement en régime établi,\n","ou reprendre l'entraînement.\n","C'est un tel objet qui va être repésenté graphiquement, car il dépend d'un environnement\n","qu'on pourra représenter, et contient des games qui pourront être représentés\n","successivement, ainsi qu'une QTable évolutive.\"\"\"\n","\n","    def __init__(self, env = E, alpha = 0.75, gamma = 0.9):\n","        self.env = env #le terrain de jeu\n","\n","        #la partie\n","        self.G = game(self.env, self, 1, 20)#juste pour ne pas qu'il y ait d'erreur ; cette partie ne sera jamais jouée et sera écrasée\n","\n","        #la QTable du joueur , prenant en paramètre les coordonnées puis l'action\n","        self.QTable=[[{action:0 for action in env.states[x][y].possible_actions} for y in range(self.env.len)]for x in range(self.env.len)]\n","        #grosse pythonerie, prendre le temps de lire. Et on initialise des Q valeurs que pour les actions possibles à chaque état\n","\n","        #notre joueur doit avoir un QTable limite unique (dépend de l'environnement et des paramètres alpha et gamma) vers laquelle on tend en l'entraînant\n","        self.a = alpha\n","        self.g = gamma\n","\n","        self.train_nb = 0 #nombre de parties jouées\n","\n","        self.tempo=True #pour savoir si on affiche les parties lentement pour voir les actions ou si on les joue très vite\n","\n","        try:\n","            self.env.canvas.delete('arrow')#enlever les éventuelles flèches d'un précédent joueur\n","        except:\n","            pass\n","\n","\n","    def new_game(self, epsilon, tau):\n","        self.G = game(self.env, self, epsilon, tau)\n","\n","\n","    def train(self, N = 200, stop = True , threshold = 0.001, train_type = 'inverseroot', method='e_greedy', epsilon=0.1, tau=20): #on train sur plusieurs parties en explorant d'une certaine façon\n","        \"\"\"Fait jouer des joueurs successifs et arrête l'entraînement soit quand le deltamax de la dernière partie jouée est inférieur au seuil (si stop est true)\n","    soit après avoir fait jouer N joueurs. La méthode renseigne comment vont être jouées les parties, et le type d'entraînement permet un raffinement de la\n","    manière dont vont être générée les taux d'exploration des parties si on utilise epsilon-greedy. Par exemple, on peut sélectionner une décroissance linéaire.\"\"\"\n","\n","        self.new_game(1, tau)#le premier joueur est entièrement aléatoire si on utilise la méthode epsilon-greedy, et est normal avec softmax\n","        deltamax = math.inf\n","\n","        while (stop and deltamax>threshold and self.train_nb<N) or (stop==False and self.train_nb<N):\n","            #on joue des parties\n","\n","            #position du joueur sur le canvas\n","            self.graphic_update()\n","\n","            #pour éventuellement jouer les parties lentement\n","            if self.tempo:time.sleep(0.05)\n","\n","            while self.G.is_over==False:\n","                #on joue un coup\n","                self.G.play(method=method)\n","\n","                #actualisation des informations visuelles\n","                self.show_arrows()\n","                self.graphic_update()\n","\n","                if self.tempo:time.sleep(0.05)\n","\n","            if self.tempo:time.sleep(0.5)\n","\n","            #on en récupère le deltamax\n","            deltamax=self.G.deltamax\n","\n","            #on incrémente le compteur de parties jouées\n","            self.train_nb+=1\n","\n","            #nouvelle partie !\n","            if train_type=='linear':\n","                self.new_game(1-self.train_nb/N, tau)\n","            elif train_type=='constant':\n","                self.new_game(epsilon, tau)\n","            elif train_type=='exponential':\n","                self.new_game(math.exp(-self.train_nb), tau)\n","            elif train_type=='quadratic':\n","                self.new_game((1-self.train_nb/N)**2, tau)\n","            elif train_type=='inverseroot':\n","                self.new_game(1/math.sqrt(self.train_nb + 1), tau)\n","\n","        #notre QTable a été entraînée !\n","        self.env.canvas.delete('playerpos')\n","        print(self.train_nb)\n","\n","\n","\n","    def switch_tempo(self): #methode pour interaction via IHM\n","        self.tempo = not self.tempo\n","\n","\n","    def play(self, epsilon, learning=False, method='e_greedy', tau = 20):\n","        #joue une partie, par défaut optimale, sans update la QTable\n","        self.new_game(epsilon, tau)#balec si learning==False\n","        self.graphic_update()\n","        time.sleep(0.1)\n","        while self.G.is_over==False:\n","            self.G.play(learning, method)\n","            self.graphic_update()\n","            time.sleep(0.1)\n","        self.env.canvas.delete('playerpos')\n","\n","        return self.G.score\n","\n","\n","    def update_QTable(self, previous_state, action, state, reward):\n","        self.QTable[previous_state.x][previous_state.y][action] = (1-self.a)*self.QTable[previous_state.x][previous_state.y][action] + self.a *(reward + self.g * max(list(self.QTable[state.x][state.y].values())))\n","\n","    def graphic_update(self):\n","        self.env.canvas.delete('playerpos')\n","        self.env.canvas.create_oval(self.G.state.x*self.env.step -10 + self.env.step/2,\n","                                            (self.G.state.y)*self.env.step-10 + self.env.step/2,\n","                                            self.G.state.x*self.env.step+10 + self.env.step/2,\n","                                            (self.G.state.y)*self.env.step+10 + self.env.step/2,\n","                                            fill = 'red', tag='playerpos')\n","        self.env.canvas.pack()\n","        self.env.root.update_idletasks()\n","        self.env.root.update()\n","\n","\n","    def show_QTable(self): #à optimiser comme pour les flèches, là c'est giga lourd\n","        self.env.canvas.delete('texte')\n","        for i in range(self.env.len):\n","            for j in range(self.env.len):\n","                s=self.env.states[i][j]\n","                if 'Left' in s.possible_actions:\n","                    self.env.canvas.create_text(2+i*self.env.step, 2+(j+0.5)*self.env.step, text=str(int(self.QTable[i][j]['Left'])), anchor='w', tag='texte')\n","                if 'Right' in s.possible_actions:\n","                    self.env.canvas.create_text(2+(i+1)*self.env.step, 2+(j+0.5)*self.env.step, text=str(int(self.QTable[i][j]['Right'])), anchor='e', tag='texte')\n","                if 'Up' in s.possible_actions:\n","                    self.env.canvas.create_text(2+(i+0.5)*self.env.step, 2+j*self.env.step, text=str(int(self.QTable[i][j]['Up'])), anchor='n', tag='texte')\n","                if 'Down' in s.possible_actions:\n","                    self.env.canvas.create_text(2+(i+0.5)*self.env.step, 2+(j+1)*self.env.step, text=str(int(self.QTable[i][j]['Down'])), anchor='s', tag='texte')\n","        return\n","\n","    def show_arrows(self):\n","        i, j = self.G.state.x, self.G.state.y\n","        self.env.canvas.delete('flèche_%d_%d'%(i, j))\n","        if 'Left' in self.G.best_actions():\n","            self.env.canvas.create_line(2+(i+0.25)*self.env.step, 2+(j+0.5)*self.env.step,2+(i-0.25)*self.env.step, 2+(j+0.5)*self.env.step, arrow='last', tag=('flèche_%d_%d'%(i, j), 'arrow'))\n","        if 'Right' in self.G.best_actions():\n","            self.env.canvas.create_line(2+(i+0.75)*self.env.step, 2+(j+0.5)*self.env.step,2+(i+1.25)*self.env.step, 2+(j+0.5)*self.env.step, arrow='last', tag=('flèche_%d_%d'%(i, j), 'arrow'))\n","        if 'Up' in self.G.best_actions():\n","            self.env.canvas.create_line(2+(i+0.5)*self.env.step, 2+(j+0.25)*self.env.step,2+(i+0.5)*self.env.step, 2+(j-0.25)*self.env.step, arrow='last', tag=('flèche_%d_%d'%(i, j), 'arrow'))\n","        if 'Down' in self.G.best_actions():\n","            self.env.canvas.create_line(2+(i+0.5)*self.env.step, 2+(j+0.75)*self.env.step,2+(i+0.5)*self.env.step, 2+(j+1.25)*self.env.step, arrow='last', tag=('flèche_%d_%d'%(i, j), 'arrow'))\n","        return\n","\n","player1=player()\n","\n","def new_player():\n","    global player1\n","    player1=player()\n","    return\n","def train_player1():\n","    global player1\n","    player1.train()\n","    return\n","def switch_tempo_player1():\n","    global player1\n","    player1.switch_tempo()\n","    return\n","def play_player1():\n","    global player1\n","    player1.play(scale_epsilon.get())\n","    return\n","\n","\n","\n","frame = Frame(E.root, height = E.size+14, width = 100)\n","frame.pack(side='right')\n","\n","switch_tempo_player1()\n","train_player1()\n","switch_tempo_player1()\n","\n","scale_epsilon = Scale(frame, orient='horizontal', from_=0, to=1, resolution=0.1, label='Régler epsilon:')\n","scale_epsilon.pack()\n","\n","button_play=Button(frame, command=play_player1, text='Jouer une partie')\n","button_play.pack()\n","\n","\n","E.root.mainloop()\n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TclError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fef7034ba150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;31m#terminal_states=[[1,0],[6,0],[1,1],[4,2]], rewards=[100, -1000, -1000, -1000, 10, -1000, 2, 2], start_point=[1, 5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewarded_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-fef7034ba150>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, rewarded_states, rewards, terminal_states, forbidden_states, start_point, max_plays, move_penalty)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Jeu 2D interactif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"]}]},{"cell_type":"markdown","metadata":{"id":"8rXU9nMrrPwF","colab_type":"text"},"source":["### Courbe de convergence selon epsilon et tau ## \n","\n"]},{"cell_type":"markdown","metadata":{"id":"UR8RHY32KYcj","colab_type":"text"},"source":["Voici une étude de l'efficacité des deux méthodes un jeu 2D 7x7 que vous trouverez en annexe, selon les paramètres. Il est représenté ci-dessous. Le but est de faire le meilleur score en 30 coups, sachant que les cases rouges sont mortelles et très punitives en terme de récompense.\n","Ce jeu est intéressant dans le sens où si le joueur explore trop peu, il se laissera piéger par la facilité de la récompense des 2 à droite, mais ensuite il découvrira le 100 en haut à gauche, plus difficile d'accès.\n","\n","\n","![jeu7x7](https://image.noelshack.com/fichiers/2019/23/3/1559763588-jeu-7x7.png)\n","\n","![egreedy](https://image.noelshack.com/fichiers/2019/23/3/1559763578-e-greedy-100-joueurs-annote.png)\n","\n","![softmax](https://image.noelshack.com/fichiers/2019/23/3/1559763583-softmax-100-joueurs-annote.png)\n","\n","\n","Il est important de garder à l'esprit que ces méthodes sont basiques et vous en trouverez de plus complexes en explorant le code. Cependant, cela permet de voir que plus l'exploration est importante dans la stratégie comportementale, mieux le joueur entraîné saura jouer mais que l'entraînement sera plus long. Ici la méthode softmax fournit de bons résultats."]},{"cell_type":"markdown","metadata":{"id":"7A75AYworPwF","colab_type":"text"},"source":["## 4 - Exemples"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o8LSDwNRjlB9"},"source":["##Définir un environnement\n"]},{"cell_type":"markdown","metadata":{"id":"6hO6PSt9mpVM","colab_type":"text"},"source":["Dans cette partie nous allons pouvoir nous entrainer à mettre en œuvre les principes théoriques vues précédemment. Pour cela nous allons commencer avec un petit exercice visant à définir un environnement pour notre agent. Précédemment on a appris que l’outil utilisé pour représenter l’environnement était les graphes de Markov. Pour implémenter un environnement on commence par définir une classe *environment* :\n","\n","def __init__(self, n, rewarded_states=[], rewards=[], terminal_states=[], forbidden_states=[], start_point=[0, 0], max_plays = 30, move_penalty = -1):\n","\n","Ensuite on n'a plus qu’a définir les valeurs des différents paramètres :\n","\n","\n","\n","Par exemple pour représenter cette grille on définit les paramètres suivants (les coordonnées sont prises par rapport à en haut à gauche pour suivre les coordonées de tkinter):\n","\n","![Matrice Jeu ](https://media.giphy.com/media/Ss0k78Aos1uiCE6aX9/giphy.gif)\n","\n","**N**= 4\n","\n","**rewarded_states**= [ [1,1] , [2,0] ]\n","\n","**rewards**= [ -100 , 100 ]\n","\n","**terminal_states** = [ [1,1] ]\n","\n","On a alors la commande: \n","\n","E= *environment* ( n=4, rewarded_states= [ [1,1] , [2,0] ] , rewards= [ -100 , 100 ] , terminal_states = [ [1,1] ] )"]},{"cell_type":"markdown","metadata":{"id":"A3FFRSo40P6g","colab_type":"text"},"source":["###Exercice:"]},{"cell_type":"markdown","metadata":{"id":"NipgZfGB0Nz8","colab_type":"text"},"source":["A vous de compléter la commande pour définir l'environnement suivant:\n","\n","On rappelle que la classe *environment* est définie de la manière suivante: \n","\n","*def init(self, n, rewarded_states=[], rewards=[], terminal_states=[], forbidden_states=[], start_point=[0, 0], max_plays = 30, move_penalty = -1)*\n","\n","![Mat_ex](https://media.giphy.com/media/iD6jZD6Z8ryIOekyZv/giphy.gif)\n"]},{"cell_type":"code","metadata":{"id":"FOx74tF6xDxK","colab_type":"code","colab":{}},"source":["### A COMPLETER\n","\n","E = \n","\n","### CODE AFFICHAGE: ne pas toucher\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvKz9AP80XIC","colab_type":"text"},"source":["###Correction:"]},{"cell_type":"code","metadata":{"id":"mSSNx8lS0dYM","colab_type":"code","colab":{}},"source":["### A COMPLETER\n","\n","E = environnement(n=4, rewarded_states=[[1,1],[0,0],[2,0]] ,rewards=[-100,50,100] ,terminal_states=[[1,1]],forbidden_states=[[1,2],[2,2]] )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1C_3LqQorPwG","colab_type":"text"},"source":["## Jeu des allumettes"]},{"cell_type":"markdown","metadata":{"id":"EPGMjCgUrPwG","colab_type":"text"},"source":["**Principe du jeu:**\n","Le jeu se joue à deux, avec un certain nombre d'allumettes, disons 12. Chacun leur tour, les joueurs peuvent prendre 1, 2 ou 3 allumettes. Celui qui prend la dernière allumette perd.\n","\n","Il existe une méthode optimale permettant au premier joueur de gagner à tous les coups. Il suffit de toujours laisser l'adversaire avec un nombre d'allumettes congru à 1 modulo 4 (1, 5, 9, ...). Voyons si l'apprentissage par renforcement permet de retrouver cette méthode optimale, en utilisant la méthode du TD-Learning."]},{"cell_type":"markdown","metadata":{"id":"lE75pEYiy_Rw","colab_type":"text"},"source":["###Exercice:"]},{"cell_type":"markdown","metadata":{"id":"cjsQGaUfk1nf","colab_type":"text"},"source":["Le but de cet exercice est de vous initier à l'implémentation d'une fonction de valeur."]},{"cell_type":"code","metadata":{"id":"QHGgeAr2zusZ","colab_type":"code","colab":{}},"source":["###### Manque les consignes de l'exercice"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZftsIswzFA3","colab_type":"text"},"source":["###Correction:"]},{"cell_type":"markdown","metadata":{"id":"O79tqeFirPwH","colab_type":"text"},"source":["Tout d'abord, définissons une classe permettant de jouer une partie:"]},{"cell_type":"code","metadata":{"id":"l4KEoWJcrPwI","colab_type":"code","colab":{}},"source":["import math as m\n","import random\n","\n","\n","# Classe représentant une partie\n","class StickGame(object):\n","\n","    def __init__(self, nb, player1, player2):\n","        self.original_nb = nb  # nombre d'allumettes au départ\n","        self.nb = nb  # nombre d'allumettes restantes\n","        self.history = []  # historique de la partie\n","        self.player1 = player1  # joueur1\n","        self.player2 = player2  # joueur2\n","        self.alpha = 0.1        # facteur d'apprentissage\n","\n","    # Fonction représentant un tour\n","    def play_action(self):\n","        action1 = self.player1.action(self.nb)  # action du joueur 1\n","        nb_before_action = self.nb  # nombre d'allumettes au début du tour\n","        int_nb = self.nb - action1  # nombre d'allumettes après l'action 1\n","\n","        if int_nb <= 0:  # Défaite du joueur 1\n","            print('Player2 wins')\n","            self.history.append((nb_before_action, 0, 0))\n","            return 0\n","\n","        if self.player2.is_human:  # Affichage des allumettes si le joueur 2 est humain\n","            self.display(int_nb)\n","\n","        action2 = self.player2.action(int_nb)  # action du joueur 2\n","        new_nb = int_nb - action2  # nombre d'allumettes après l'action 2\n","        self.history.append((nb_before_action, int_nb, new_nb))  # ajout du tour à l'historique\n","\n","        if new_nb <= 0:  # Défaite du joueur 2\n","            print('Player1 wins')\n","            return 0\n","        else:\n","            return new_nb\n","\n","    # Afffichage des allumettes\n","    def display(self, state):\n","        print(\"| \" * state)\n","\n","    # Actualise la fonction de valeur V\n","    def update_V(self):\n","        for transition in reversed(self.history):  # Parcours à l'envers de l'historique de la partie\n","            (s, si, sp) = transition  # Un tour s -> si -> sp\n","            if si == 0:  # Défaite du joueur 1\n","                V[s] = V[s] + self.alpha * (-1 - V[s])\n","            elif sp == 0:  # Défaite du joueur 2\n","                V[s] = V[s] + self.alpha * (1 - V[s])\n","            else:\n","                V[s] = V[s] + self.alpha * (V[sp] - V[s])\n","\n","    # Lancement d'une partie\n","    def game(self):\n","        while self.nb > 0:  # Lance un tour tant qu'il reste des allumettes\n","            self.display(self.nb)\n","            self.nb = self.play_action()\n","        self.update_V()  # Actualise V\n","        self.nb = self.original_nb  # Réinitialise le nombre d'allumettes\n","        self.history = []  # Réinitialise l'historique\n","\n","    def run(self):\n","        self.game()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IMxk87EyrPwK","colab_type":"text"},"source":["Définissons maintenant une classe joueur, avec un argument permettant de désigner si le joueur est humain ou non:"]},{"cell_type":"code","metadata":{"id":"Au447j2frPwL","colab_type":"code","colab":{}},"source":["# Classe de joueur\n","class Player(object):\n","\n","    def __init__(self, is_human):\n","        self.epsilon = 1\n","        self.is_human = is_human  # True si humain, False si IA\n","\n","    def greedy_action(self, state):  # Action d'exploitation de l'IA\n","        actions = [1, 2, 3]\n","        vmin = m.inf\n","        action_min = 1\n","        for i in actions:\n","            if state - i > 0 and V[state - i] < vmin:\n","                vmin = V[state - i]\n","                action_min = i\n","        return action_min\n","\n","    def random_action(self, state):  # Action d'exploration de l'IA\n","        if state == 1:\n","            return 1\n","        elif state <= 3:\n","            return random.randint(1, state - 1)\n","        else:\n","            return random.randint(1, 3)\n","\n","    def human_action(self):  # Demande d'entrée de l'action humaine\n","        action = int(input('>:'))\n","        if action not in [1, 2, 3]:\n","            action = int(input('>:'))\n","        return action\n","\n","    def action(self,\n","               state):  # Choisis l'action du joueur avec une proba epsilon de choisir l'action d'exploration pour l'IA\n","        if self.is_human:\n","            return self.human_action()\n","\n","        elif random.uniform(0, 1) < self.epsilon:\n","            return self.random_action(state)\n","\n","        else:\n","            return self.greedy_action(state)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_Gyit8jrPwN","colab_type":"text"},"source":["Nous pouvons maintenant jouer une partie:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PweW-D0SrPwN","colab_type":"code","colab":{}},"source":["nb = 12\n","Player_1 = Player(True)\n","Player_2 = Player(True)\n","V = {}\n","for i in range(nb):\n","    V[i+1] = 0\n","\n","StickGame(nb, Player_1, Player_2).run()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZIxfCa8rPwQ","colab_type":"text"},"source":["Essayons maintenant d'apprendre à une IA à jouer. Dans cette phase d'apprentissage, nous ferons affronter deux IA l'une contre l'autre pendant un grand nombre de parties (500), ce qui mettra à jour la fonction de valeur.\n","\n","L'IA peut jouer de deux manières: par exploration, c'est à dire en choisissant un nombre aléatoire d'allumettes, ou par exploitation, en prenant un nombre d'allumettes qui va minimiser la fonction de valeur pour l'adversaire. Ce taux exploration/exploitation est représenté par $\\epsilon$. Pendant l'apprentissage, on commence avec $\\epsilon = 1$, c'est-à-dire uniquement de l'exploration, puis on diminue progressivement jusqu'à $\\epsilon = 0$, pour n'avoir que de l'exploitation de la part de l'IA."]},{"cell_type":"code","metadata":{"id":"FCtD2T-2rPwR","colab_type":"code","colab":{}},"source":["IA_Player = Player(False)\n","IA_Player2 = Player(False)\n","epsilon = 1\n","learning_nb = 500\n","LearningGame = StickGame(nb, IA_Player, IA_Player2)\n","\n","for k in range(learning_nb):\n","    print('Partie n° ', k+1)\n","    LearningGame.run()\n","    epsilon -= 1 / learning_nb\n","    IA_Player.epsilon = epsilon\n","    IA_Player2.epsilon = epsilon\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejq1-L-KrPwT","colab_type":"text"},"source":["A toi de jouer maintenant!"]},{"cell_type":"code","metadata":{"id":"yWd7yA75rPwU","colab_type":"code","colab":{}},"source":["Human_Player = Player(True)\n","StickGame = StickGame(nb, Human_Player, IA_Player)\n","for i in range(10):\n","    StickGame.run()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKJRV2epy0v-","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"20tHsMKxrPwW","colab_type":"text"},"source":["##Pendule inversé"]},{"cell_type":"markdown","metadata":{"id":"sFp_aeSFrPwW","colab_type":"text"},"source":["**Principe:** Le pendule est constitué d'un module pouvant se déplacer horizontalement, et d'une tige rigide fixée au module par son extrémité. Le but est de faire tenir la tige en équilibre au dessus du module, à la verticale.\n","\n","Pour coder ce pendule inversé, nous utilisons un module python dédié à l'apprentissage par renforcement: gym."]},{"cell_type":"code","metadata":{"id":"hgIem79RrPwX","colab_type":"code","colab":{}},"source":["import sys\n","!{sys.executable} -m pip install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"DkYmHCHOrPwa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":385},"outputId":"43c5c4f4-dcf8-46a2-f2ca-7bc2bb383d9c","executionInfo":{"status":"error","timestamp":1559770551583,"user_tz":-120,"elapsed":1702,"user":{"displayName":"antoine houdouin","photoUrl":"","userId":"05070167574197619815"}}},"source":["import gym\n","import gym.spaces\n","import numpy as np\n","import math\n","import random\n","from collections import deque\n","\n","# Création de l'environnement à partir de la bibliothèque gym\n","env = gym.make('CartPole-v0')\n","\n","\n","class CartPole(object):\n","\n","    def __init__(self, buckets=(1, 1, 6, 3), n_episodes=1000, solved_t=195,\n","                 min_epsilon=0.1, min_alpha=0.1, gamma=0.99):\n","        self.buckets = buckets              # (position, vitesse, angle, vitesse angulaire)\n","        self.n_episodes = n_episodes        # nombre d'épisodes d'entrainement\n","        self.min_alpha = min_alpha\n","        self.min_epsilon = min_epsilon\n","        self.gamma = gamma\n","        self.solved_t = solved_t            # limite de temps pour un épisode\n","\n","        self.Q_table = np.zeros(self.buckets + (env.action_space.n,))  # Table de la fonction de valeur Q\n","\n","    def discretize_state(self, state):\n","        upper_bounds = env.observation_space.high   # Limites de dimension\n","        lower_bounds = env.observation_space.low\n","\n","        upper_bounds[1] = 0.5                       # Limites de vitesse et de vitesse angulaire\n","        upper_bounds[3] = math.radians(50)\n","        lower_bounds[1] = -0.5\n","        lower_bounds[3] = -math.radians(50)\n","\n","        # Discrétisation des dimensions\n","        width = [upper_bounds[i] - lower_bounds[i] for i in range(len(state))]\n","        ratios = [(state[i] - lower_bounds[i]) / width[i] for i in range(len(state))]\n","        bucket_indices = [int(round(ratios[i] * (self.buckets[i] - 1))) for i in range(len(state))]\n","        bucket_indices = [max(0, min(bucket_indices[i], self.buckets[i] - 1)) for i in range(len(state))]\n","\n","        return tuple(bucket_indices)\n","\n","    # Implémentation de la méthode epsilon-gloutonne\n","    def select_action(self, state, epsilon):\n","        if random.random() <= epsilon:\n","            return env.action_space.sample()  # exploration\n","        else:\n","            return np.argmax(self.Q_table[state])  # exploitation\n","\n","    # Choisis un epsilon entre min_epsilon et 1\n","    def get_epsilon(self, episode_number):\n","        return max(self.min_epsilon, min(1, 1 - math.log10((episode_number + 1) / 25)))\n","\n","    # Choisis un alpha entre min_alpha et 1\n","    def get_alpha(self, episode_number):\n","        return max(self.min_alpha, min(1, 1 - math.log10((episode_number + 1) / 25)))\n","\n","    # Actualisation de la table de valeur de Q selon la méthode de Q-Learning\n","    def update_table(self, old_state, action, reward, new_state, alpha):\n","        new_state_Q_value = np.max(self.Q_table[new_state])\n","        self.Q_table[old_state][action] += alpha * (\n","                    reward + self.gamma * new_state_Q_value - self.Q_table[old_state][action])\n","\n","    def run(self):\n","        # Continue les episodes tant que la récompense moyenne de 100 episodes est en dessous de solved_t\n","        scores = deque(maxlen=100)\n","\n","        for episode in range(self.n_episodes):\n","            #Déroulé d'un épisode\n","            done = False\n","            alpha = self.get_alpha(episode)\n","            epsilon = self.get_epsilon(episode)\n","            episode_reward = 0                      # récompense = temps de l'épisode\n","\n","            obs = env.reset()                       # réinitialisation de l'environnement\n","            curr_state = self.discretize_state(obs)\n","\n","            while not done:\n","                env.render()\n","                action = self.select_action(curr_state, epsilon)\n","                obs, reward, done, info = env.step(action)\n","                new_state = self.discretize_state(obs)\n","\n","                self.update_table(curr_state, action, reward, new_state, alpha)\n","                curr_state = new_state\n","                episode_reward += reward\n","\n","            scores.append(episode_reward)\n","            mean_reward = np.mean(scores)\n","\n","            # Affichage des épisodes\n","            if mean_reward > self.solved_t and (episode + 1) >= 100:\n","                print(\"Ran {} episodes, solved after {} trials\".format(episode + 1, episode + 1 - 100))\n","                return episode + 1 - 100\n","            elif (episode + 1) % 50 == 0 and (episode + 1) >= 100:\n","                print(\"Episode number: {}, mean reward over past 100 episodes is {}\".format(episode + 1, mean_reward))\n","            else:\n","                print(\"Episode {}, reward {}\".format(episode + 1, episode_reward))\n","\n","\n","if __name__ == \"__main__\":\n","    cartpole = CartPole()\n","    cartpole.run()\n","\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NoSuchDisplayException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-07e92325cff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mcartpole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-07e92325cff9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""]}]},{"cell_type":"code","metadata":{"id":"1bnHXIGuw8qF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}